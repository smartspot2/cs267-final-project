import time
from typing import Any, Callable, Optional

import torch
import torch.distributed as dist
from diffusers.schedulers.scheduling_ddim import DDIMScheduler, DDIMSchedulerOutput
from diffusers.schedulers.scheduling_ddpm import DDPMScheduler, DDPMSchedulerOutput
from diffusers.utils.pil_utils import pt_to_pil

import utils.device
from denoisers.base import Denoiser
from models.base import PretrainedModel


class ParadigmsDenoiser(Denoiser):
    def __init__(self, model: PretrainedModel):
        super().__init__(model)

        # self.scheduler = ParaDDPMScheduler().from_pretrained(
        #     model.model_id, subfolder="scheduler", timestep_spacing="trailing"
        # )
        self.scheduler = ParaDDIMScheduler().from_pretrained(
            model.model_id, subfolder="scheduler", timestep_spacing="trailing"
        )

    @torch.no_grad()
    def denoise(
        self,
        initial_latents: torch.FloatTensor,
        prompt: str | list[str],
        height: int,
        width: int,
        num_inference_steps: int = 50,
        parallel: int = 10,
        tolerance: float = 0.1,
        guidance_scale: float = 7.5,
        negative_prompt: Optional[str | list[str]] = None,
        num_images_per_prompt: int = 1,
        eta: float = 0.0,
        generator: Optional[torch.Generator | list[torch.Generator]] = None,
        prompt_embeds: Optional[torch.FloatTensor] = None,
        negative_prompt_embeds: Optional[torch.FloatTensor] = None,
        output_type: str = "pil",
        return_dict: bool = True,
        # callback: Optional[Callable[[int, int, torch.FloatTensor], None]] = None,
        callback_steps: int = 1,
        cross_attention_kwargs: Optional[dict[str, Any]] = None,
        full_return: bool = False,
        intermediate_image_path: Optional[str] = None,
    ):
        r"""
        Function invoked when calling the pipeline for generation.

        Args:
            prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts to guide the image generation. If not defined, one has to pass `prompt_embeds`.
                instead.
            height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The height in pixels of the generated image.
            width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):
                The width in pixels of the generated image.
            num_inference_steps (`int`, *optional*, defaults to 50):
                The number of denoising steps. More denoising steps usually lead to a higher quality image at the
                expense of slower inference.
            guidance_scale (`float`, *optional*, defaults to 7.5):
                Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).
                `guidance_scale` is defined as `w` of equation 2. of [Imagen
                Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >
                1`. Higher guidance scale encourages to generate images that are closely linked to the text `prompt`,
                usually at the expense of lower image quality.
            negative_prompt (`str` or `List[str]`, *optional*):
                The prompt or prompts not to guide the image generation. If not defined, one has to pass
                `negative_prompt_embeds`. instead. If not defined, one has to pass `negative_prompt_embeds`. instead.
                Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).
            num_images_per_prompt (`int`, *optional*, defaults to 1):
                The number of images to generate per prompt.
            eta (`float`, *optional*, defaults to 0.0):
                Corresponds to parameter eta (η) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to
                [`schedulers.DDIMScheduler`], will be ignored for others.
            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):
                One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)
                to make generation deterministic.
            latents (`torch.FloatTensor`, *optional*):
                Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
                tensor will ge generated by sampling using the supplied random `generator`.
            prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not
                provided, text embeddings will be generated from `prompt` input argument.
            negative_prompt_embeds (`torch.FloatTensor`, *optional*):
                Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt
                weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input
                argument.
            output_type (`str`, *optional*, defaults to `"pil"`):
                The output format of the generate image. Choose between
                [PIL](https://pillow.readthedocs.io/en/stable/): `PIL.Image.Image` or `np.array`.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a
                plain tuple.
            callback (`Callable`, *optional*):
                A function that will be called every `callback_steps` steps during inference. The function will be
                called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.
            callback_steps (`int`, *optional*, defaults to 1):
                The frequency at which the `callback` function will be called. If not specified, the callback will be
                called at every step.
            cross_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttnProcessor` as defined under
                `self.processor` in
                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).

        Examples:

        Returns:
            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] or `tuple`:
            [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] if `return_dict` is True, otherwise a `tuple.
            When returning a tuple, the first element is a list with the generated images, and the second element is a
            list of `bool`s denoting whether the corresponding generated image likely represents "not-safe-for-work"
            (nsfw) content, according to the `safety_checker`.
        """

        # TODO: no intermediate image saving for now (paradigms not used in search)
        del intermediate_image_path

        print("parallel pipeline!", flush=True)
        cur_rank = dist.get_rank()
        n_ranks = dist.get_world_size()
        assert cur_rank == 0, "Main paradigms manager node must be run on rank 0"

        # 0. Default height and width to unet
        # height = height or self.unet.config.sample_size * self.vae_scale_factor
        # width = width or self.unet.config.sample_size * self.vae_scale_factor

        # 1. Check inputs. Raise error if not correct
        self.model.pipeline.check_inputs(
            prompt,
            height,
            width,
            callback_steps,
            negative_prompt,
            prompt_embeds,
            negative_prompt_embeds,
        )

        # 2. Define call parameters
        if prompt is not None and isinstance(prompt, str):
            batch_size = 1
        elif prompt is not None and isinstance(prompt, list):
            batch_size = len(prompt)
        else:
            batch_size = prompt_embeds.shape[0]

        device = utils.device.DEVICE
        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)
        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`
        # corresponds to doing no classifier free guidance.
        do_classifier_free_guidance = guidance_scale > 1.0

        # 3. Encode input prompt
        prompt_embeds_tuple = self.model.encode_prompt(
            prompt,
            device,
            num_images_per_prompt,
            do_classifier_free_guidance,
            negative_prompt,
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_prompt_embeds,
        )
        # concatenate for backwards comp (NOTE: because here we use encode_prompt rather than _encode_prompt)
        prompt_embeds = torch.cat([prompt_embeds_tuple[1], prompt_embeds_tuple[0]])

        # 4. Prepare timesteps
        self.scheduler.set_timesteps(num_inference_steps, device=device)
        scheduler = self.scheduler

        # 5. Prepare latent variables
        # num_channels_latents = self.model.pipeline.unet.in_channels
        # latents = self.model.pipeline.prepare_latents(
        #     batch_size * num_images_per_prompt,
        #     num_channels_latents,
        #     height,
        #     width,
        #     prompt_embeds.dtype,
        #     device,
        #     generator,
        #     # initial_latents,
        # )
        assert initial_latents.shape[0] == batch_size * num_images_per_prompt
        latents = initial_latents.to(device)

        # 6. Prepare extra step kwargs.
        # extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)

        # 7. Denoising loop
        # print(scheduler.timesteps)
        stats_pass_count = 0
        stats_flop_count = 0
        parallel = min(parallel, len(scheduler.timesteps))

        begin_idx: int = 0
        end_idx: int = parallel
        latents_time_evolution_buffer = torch.stack(
            [latents] * (len(scheduler.timesteps) + 1)
        )

        # We specify the error tolerance as a ratio of the scheduler's noise magnitude. We similarly compute the error tolerance
        # outside of the denoising loop to avoid recomputing it at every step.
        # We will be dividing the norm of the noise, so we store its inverse here to avoid a division at every step.
        noise_array = torch.zeros_like(latents_time_evolution_buffer)
        for j in range(len(scheduler.timesteps)):
            # TODO: replace this noise generation with inputs
            base_noise = torch.randn_like(latents)
            noise = (
                self.scheduler._get_variance(scheduler.timesteps[j]) ** 0.5
            ) * base_noise
            noise_array[j] = noise.clone()

        # We specify the error tolerance as a ratio of the scheduler's noise magnitude. We similarly compute the error tolerance
        # outside of the denoising loop to avoid recomputing it at every step.
        # We will be dividing the norm of the noise, so we store its inverse here to avoid a division at every step.
        inverse_variance_norm = 1.0 / torch.tensor(
            [
                scheduler._get_variance(scheduler.timesteps[j])
                for j in range(len(scheduler.timesteps))
            ]
            + [0]
        ).to(noise_array.device)
        latent_dim = noise_array[0, 0].numel()
        inverse_variance_norm = inverse_variance_norm[:, None] / latent_dim

        scaled_tolerance = tolerance**2

        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)

        start.record()

        while begin_idx < len(scheduler.timesteps):
            # these have shape (parallel_dim, 2*batch_size, ...)
            # parallel_len is at most parallel, but could be less if we are at the end of the timesteps
            # we are processing batch window of timesteps spanning [begin_idx, end_idx)
            parallel_len = end_idx - begin_idx

            block_prompt_embeds = torch.stack([prompt_embeds] * parallel_len)
            block_latents = latents_time_evolution_buffer[begin_idx:end_idx]
            block_t = scheduler.timesteps[begin_idx:end_idx, None].repeat(
                1, batch_size * num_images_per_prompt
            )
            t_vec = block_t
            if do_classifier_free_guidance:
                t_vec = t_vec.repeat(1, 2)

            # expand the latents if we are doing classifier free guidance
            latent_model_input = (
                torch.cat([block_latents] * 2, dim=1)
                if do_classifier_free_guidance
                else block_latents
            )
            latent_model_input = self.scheduler.scale_model_input(
                latent_model_input, t_vec
            )

            # TODO: add GPU parallelism
            # if parallel_len is small, no need to use multiple GPUs
            # net = self.wrapped_unet if parallel_len > 3 else self.unet
            # predict the noise residual
            # model_output = net(
            #     latent_model_input.flatten(0, 1),
            #     t_vec.flatten(0, 1),
            #     encoder_hidden_states=block_prompt_embeds.flatten(0, 1),
            #     cross_attention_kwargs=cross_attention_kwargs,
            #     return_dict=False,
            # )[0]

            # NOTE: old code for parallel = 1
            # expected_model_output = self.model.forward(
            #     latent_model_input.flatten(0, 1),
            #     t_vec.flatten(0, 1),
            #     encoder_hidden_states=block_prompt_embeds.flatten(0, 1),
            #     cross_attention_kwargs=cross_attention_kwargs,
            #     # return_dict=False,
            # )[0]

            # split into n-1 chunks; skip self
            chunks = torch.arange(parallel_len).tensor_split(n_ranks - 1)
            latent_model_input_chunks = [latent_model_input[chunk] for chunk in chunks]
            t_vec_chunks = [t_vec[chunk] for chunk in chunks]
            block_prompt_embeds_chunks = [
                block_prompt_embeds[chunk] for chunk in chunks
            ]
            num_chunks = min(parallel_len, n_ranks - 1)

            send_requests = []
            model_outputs = [None for _ in range(num_chunks)]
            for idx in range(num_chunks):  # skip rank 0 (self)
                rank = idx + 1
                # send shapes first
                send_requests.append(
                    dist.isend(
                        torch.tensor(
                            list(latent_model_input_chunks[idx].shape),
                            dtype=torch.int,
                            device=device,
                        ),
                        rank,
                    )
                )
                send_requests.append(
                    dist.isend(
                        torch.tensor(
                            list(t_vec_chunks[idx].shape),
                            dtype=torch.int,
                            device=device,
                        ),
                        rank,
                    )
                )
                send_requests.append(
                    dist.isend(
                        torch.tensor(
                            list(block_prompt_embeds_chunks[idx].shape),
                            dtype=torch.int,
                            device=device,
                        ),
                        rank,
                    )
                )

                # then send actual tensors
                send_requests.append(dist.isend(latent_model_input_chunks[idx], rank))
                send_requests.append(dist.isend(t_vec_chunks[idx], rank))
                send_requests.append(dist.isend(block_prompt_embeds_chunks[idx], rank))

                # prepare model output list for next gather
                # input_shape = latent_model_input_chunks[idx].shape
                # output_shape = (input_shape[0] * input_shape[1], *input_shape[2:])
                model_outputs[idx] = torch.zeros_like(
                    latent_model_input_chunks[idx].flatten(0, 1), device=device
                )
                # NOTE: cross_attention_kwargs not communicated

            # wait on tensor send requests
            print("Waiting on send requests...")
            for req in send_requests:
                req.wait()

            recv_requests = []
            for idx in range(num_chunks):
                rank = idx + 1
                recv_requests.append(dist.irecv(model_outputs[idx], rank))

            # wait on all receives to finish
            print("Waiting on recv requests...")
            for req in recv_requests:
                req.wait()

            # concatenate all model outputs
            model_output = torch.cat(model_outputs, dim=0)

            # synchronize? TODO: not sure if necessary
            # torch.cuda.synchronize()

            # print(torch.allclose(expected_model_output, model_output))
            # print(torch.norm(expected_model_output - model_output))
            # model_output = expected_model_output

            per_latent_shape = model_output.shape[1:]
            if do_classifier_free_guidance:
                model_output = model_output.reshape(
                    parallel_len,
                    2,
                    batch_size * num_images_per_prompt,
                    *per_latent_shape,
                )
                noise_pred_uncond, noise_pred_text = (
                    model_output[:, 0],
                    model_output[:, 1],
                )
                model_output = noise_pred_uncond + guidance_scale * (
                    noise_pred_text - noise_pred_uncond
                )
            model_output = model_output.reshape(
                parallel_len * batch_size * num_images_per_prompt, *per_latent_shape
            )

            block_latents_denoise = scheduler.batch_step_no_noise(
                model_output=model_output,
                timesteps=block_t.flatten(0, 1),
                sample=block_latents.flatten(0, 1),
                # **extra_step_kwargs,
            ).reshape(block_latents.shape)

            # back to shape (parallel_dim, batch_size, ...)
            # now we want to add the pre-sampled noise
            # parallel sampling algorithm requires computing the cumulative drift from the beginning
            # of the window, so we need to compute cumulative sum of the deltas and the pre-sampled noises.
            delta = block_latents_denoise - block_latents
            cumulative_delta = torch.cumsum(delta, dim=0)
            cumulative_noise = torch.cumsum(noise_array[begin_idx:end_idx], dim=0)

            # if we are using an ODE-like scheduler (like DDIM), we don't want to add noise
            if scheduler._is_ode_scheduler:
                cumulative_noise = 0

            block_latents_new = (
                latents_time_evolution_buffer[begin_idx][None,]
                + cumulative_delta
                + cumulative_noise
            )
            cur_error_vec = (
                block_latents_new
                - latents_time_evolution_buffer[begin_idx + 1 : end_idx + 1]
            ).reshape(parallel_len, batch_size * num_images_per_prompt, -1)
            cur_error = torch.linalg.norm(cur_error_vec, dim=-1).pow(2)
            error_ratio = cur_error * inverse_variance_norm[begin_idx + 1 : end_idx + 1]

            # find the first index of the vector error_ratio that is greater than error tolerance
            # we can shift the window for the next iteration up to this index
            error_ratio = torch.nn.functional.pad(
                error_ratio, (0, 0, 0, 1), value=1e9
            )  # handle the case when everything is below ratio, by padding the end of parallel_len dimension
            any_error_at_time = torch.max(
                error_ratio > scaled_tolerance, dim=1
            ).values.int()
            ind = torch.argmax(any_error_at_time).item()

            # compute the new begin and end idxs for the window
            new_begin_idx = begin_idx + min(1 + ind, parallel)
            new_end_idx = min(new_begin_idx + parallel, len(scheduler.timesteps))

            # store the computed latents for the current window in the global buffer
            latents_time_evolution_buffer[begin_idx + 1 : end_idx + 1] = (
                block_latents_new
            )
            # initialize the new sliding window latents with the end of the current window,
            # should be better than random initialization
            latents_time_evolution_buffer[
                end_idx : new_end_idx + 1
            ] = latents_time_evolution_buffer[end_idx][
                None,
            ]

            begin_idx = new_begin_idx
            end_idx = new_end_idx

            stats_pass_count += 1
            stats_flop_count += parallel_len

        latents = latents_time_evolution_buffer[-1]

        print("pass count", stats_pass_count)
        print("flop count", stats_flop_count)

        end.record()

        # stop all workers (send all zeroes as the first shape)
        print("Stopping workers...")
        send_requests = []
        for rank in range(1, n_ranks):
            send_requests.append(
                dist.isend(torch.zeros(5, dtype=torch.int, device=device), rank)
            )
        for req in send_requests:
            req.wait()

        # Waits for everything to finish running
        torch.cuda.synchronize()

        print(start.elapsed_time(end))
        print("done", flush=True)

        stats = {
            "pass_count": stats_pass_count,
            "flops_count": stats_flop_count,
            "time": start.elapsed_time(end),
        }

        # def process_image(latents):
        #     if output_type == "latent":
        #         image = latents
        #         has_nsfw_concept = None
        #     elif output_type == "pil":
        #         # 8. Post-processing
        #         # print("post-processing", flush=True)
        #         image = self.decode_latents(latents)
        #
        #         # 9. Run safety checker
        #         # print("safety check", flush=True)
        #         image, has_nsfw_concept = self.run_safety_checker(
        #             image, device, prompt_embeds.dtype
        #         )
        #
        #         # 10. Convert to PIL
        #         # print("conver to PIL", flush=True)
        #         image = self.numpy_to_pil(image)
        #     else:
        #         # 8. Post-processing
        #         image = self.decode_latents(latents)
        #
        #         # 9. Run safety checker
        #         image, has_nsfw_concept = self.run_safety_checker(
        #             image, device, prompt_embeds.dtype
        #         )
        #
        #     # Offload last model to CPU
        #     if (
        #         hasattr(self, "final_offload_hook")
        #         and self.final_offload_hook is not None
        #     ):
        #         print("offload hook", flush=True)
        #         self.final_offload_hook.offload()
        #
        #     return image, has_nsfw_concept

        # if full_return:
        #     output = [
        #         process_image(latents) for latents in latents_time_evolution_buffer
        #     ]
        #
        #     if not return_dict:
        #         return [
        #             (image, has_nsfw_concept) for (image, has_nsfw_concept) in output
        #         ]
        #
        #     return [
        #         StableDiffusionPipelineOutput(
        #             images=image, nsfw_content_detected=has_nsfw_concept
        #         )
        #         for (image, has_nsfw_concept) in output
        #     ]
        # else:
        #     (image, has_nsfw_concept) = process_image(latents)
        #
        #     if not return_dict:
        #         return (image, has_nsfw_concept)
        #
        #     return (
        #         StableDiffusionPipelineOutput(
        #             images=image, nsfw_content_detected=has_nsfw_concept
        #         ),
        #         stats,
        #     )

        image = self.model.decode_image(latents)[0]
        image = self.model.postprocess_image(image)
        # return image, stats
        print(stats)
        print(image)
        return image

    def worker(
        self,
        cross_attention_kwargs: Optional[dict[str, Any]] = None,
    ):
        """
        Worker thread for paradigms.

        Important things to keep in mind:
        - number of dimensions must match exactly with expected
        - tensor dtypes must match exactly with expected
        """
        device = utils.device.DEVICE

        while True:
            # receive shapes first

            # (parallel, batch, channels, height, width)
            latent_model_input_shape = torch.zeros(5, dtype=torch.int, device=device)
            # (parallel, batch)
            t_vec_shape = torch.zeros(2, dtype=torch.int, device=device)
            # (parallel, batch, channels, features)
            block_prompt_embeds_shape = torch.zeros(4, dtype=torch.int, device=device)

            print("Worker receiving shapes...")
            dist.recv(latent_model_input_shape, 0)
            if (latent_model_input_shape == 0).all():
                # stop if we receive all zeroes
                print("Exiting...")
                return

            dist.recv(t_vec_shape, 0)
            dist.recv(block_prompt_embeds_shape, 0)

            # use shapes to initialize tensors
            latent_model_input_shape = latent_model_input_shape.tolist()
            t_vec_shape = t_vec_shape.tolist()
            block_prompt_embeds_shape = block_prompt_embeds_shape.tolist()
            latent_model_input = torch.zeros(
                latent_model_input_shape, dtype=torch.float32, device=device
            )
            t_vec = torch.zeros(t_vec_shape, dtype=torch.int64, device=device)
            block_prompt_embeds = torch.zeros(
                block_prompt_embeds_shape, dtype=torch.float32, device=device
            )

            # then receive actual tensors
            print("Worker receiving tensors...")
            dist.recv(latent_model_input, 0)
            dist.recv(t_vec, 0)
            dist.recv(block_prompt_embeds, 0)

            print("Running model...")

            # run the model on the given inputs
            model_output = self.model.forward(
                latent_model_input.flatten(0, 1),
                t_vec.flatten(0, 1),
                encoder_hidden_states=block_prompt_embeds.flatten(0, 1),
                cross_attention_kwargs=cross_attention_kwargs,
                # return_dict=False,
            )[0]

            # model_output_2 = self.model.forward(
            #     latent_model_input.flatten(0, 1),
            #     t_vec.flatten(0, 1),
            #     encoder_hidden_states=block_prompt_embeds.flatten(0, 1),
            #     cross_attention_kwargs=cross_attention_kwargs,
            #     # return_dict=False,
            # )[0]
            #
            # print("two model evals close?", torch.allclose(model_output, model_output_2))

            # finally, send the model output back to the manager node
            dist.send(model_output, 0)


class ParaDDPMScheduler(DDPMScheduler):
    # NOTE: taken directly from the paradigms repository
    # careful when overriding __init__ function, can break things due to expected_keys parameter in configuration_utils
    # if necessary copy the whole init statement from parent class

    _is_ode_scheduler = False

    def batch_step_no_noise(
        self,
        model_output: torch.FloatTensor,
        timesteps: list[int],
        sample: torch.FloatTensor,
    ) -> DDPMSchedulerOutput | tuple:
        """
        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
        process from the learned model outputs (most often the predicted noise).

        Args:
            model_output (`torch.FloatTensor`): direct output from learned diffusion model.
            timestep (`int`): current discrete timestep in the diffusion chain.
            sample (`torch.FloatTensor`):
                current instance of sample being created by diffusion process.
            generator: random number generator.
            return_dict (`bool`): option for returning tuple rather than DDPMSchedulerOutput class

        Returns:
            [`~schedulers.scheduling_utils.DDPMSchedulerOutput`] or `tuple`:
            [`~schedulers.scheduling_utils.DDPMSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When
            returning a tuple, the first element is the sample tensor.

        """
        t = timesteps
        num_inference_steps = (
            self.num_inference_steps
            if self.num_inference_steps
            else self.config.num_train_timesteps
        )
        prev_t = t - self.config.num_train_timesteps // num_inference_steps

        t = t.view(-1, *([1] * (model_output.ndim - 1)))
        prev_t = prev_t.view(-1, *([1] * (model_output.ndim - 1)))

        if model_output.shape[1] == sample.shape[1] * 2 and self.variance_type in [
            "learned",
            "learned_range",
        ]:
            model_output, predicted_variance = torch.split(
                model_output, sample.shape[1], dim=1
            )
        else:
            predicted_variance = None

        # 1. compute alphas, betas
        self.alphas_cumprod = self.alphas_cumprod.to(model_output.device)
        alpha_prod_t = self.alphas_cumprod[t]
        alpha_prod_t_prev = self.alphas_cumprod[torch.clip(prev_t, min=0)]
        alpha_prod_t_prev[prev_t < 0] = torch.tensor(1.0)

        beta_prod_t = 1 - alpha_prod_t
        beta_prod_t_prev = 1 - alpha_prod_t_prev
        current_alpha_t = alpha_prod_t / alpha_prod_t_prev
        current_beta_t = 1 - current_alpha_t

        # 2. compute predicted original sample from predicted noise also called
        # "predicted x_0" of formula (15) from https://arxiv.org/pdf/2006.11239.pdf
        if self.config.prediction_type == "epsilon":
            pred_original_sample = (
                sample - beta_prod_t ** (0.5) * model_output
            ) / alpha_prod_t ** (0.5)
        elif self.config.prediction_type == "sample":
            pred_original_sample = model_output
        elif self.config.prediction_type == "v_prediction":
            pred_original_sample = (alpha_prod_t**0.5) * sample - (
                beta_prod_t**0.5
            ) * model_output
        else:
            raise ValueError(
                f"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample` or"
                " `v_prediction`  for the DDPMScheduler."
            )

        # 3. Clip or threshold "predicted x_0"
        if self.config.thresholding:
            pred_original_sample = self._threshold_sample(pred_original_sample)
        elif self.config.clip_sample:
            pred_original_sample = pred_original_sample.clamp(
                -self.config.clip_sample_range, self.config.clip_sample_range
            )

        # 4. Compute coefficients for pred_original_sample x_0 and current sample x_t
        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf
        pred_original_sample_coeff = (
            alpha_prod_t_prev ** (0.5) * current_beta_t
        ) / beta_prod_t
        current_sample_coeff = current_alpha_t ** (0.5) * beta_prod_t_prev / beta_prod_t

        # 5. Compute predicted previous sample µ_t
        # See formula (7) from https://arxiv.org/pdf/2006.11239.pdf
        pred_prev_sample = (
            pred_original_sample_coeff * pred_original_sample
            + current_sample_coeff * sample
        )

        return pred_prev_sample


class ParaDDIMScheduler(DDIMScheduler):
    # careful when overriding __init__ function, can break things due to expected_keys parameter in configuration_utils
    # if necessary copy the whole init statement from parent class

    _is_ode_scheduler = True

    def _get_variance(self, timestep, prev_timestep=None):
        prev_timestep = (
            timestep - self.config.num_train_timesteps // self.num_inference_steps
        )

        alpha_prod_t = self.alphas_cumprod[timestep]
        alpha_prod_t_prev = (
            self.alphas_cumprod[prev_timestep]
            if prev_timestep >= 0
            else self.final_alpha_cumprod
        )
        beta_prod_t = 1 - alpha_prod_t
        beta_prod_t_prev = 1 - alpha_prod_t_prev

        variance = (beta_prod_t_prev / beta_prod_t) * (
            1 - alpha_prod_t / alpha_prod_t_prev
        )

        return variance

    def _batch_get_variance(self, t, prev_t):
        alpha_prod_t = self.alphas_cumprod[t]
        alpha_prod_t_prev = self.alphas_cumprod[torch.clip(prev_t, min=0)]
        alpha_prod_t_prev[prev_t < 0] = torch.tensor(1.0)
        beta_prod_t = 1 - alpha_prod_t
        beta_prod_t_prev = 1 - alpha_prod_t_prev

        variance = (beta_prod_t_prev / beta_prod_t) * (
            1 - alpha_prod_t / alpha_prod_t_prev
        )

        return variance

    def batch_step_no_noise(
        self,
        model_output: torch.FloatTensor,
        timesteps: list[int],
        sample: torch.FloatTensor,
        eta: float = 0.0,
        use_clipped_model_output: bool = False,
        generator=None,
        variance_noise: Optional[torch.FloatTensor] = None,
        return_dict: bool = True,
    ) -> DDIMSchedulerOutput | tuple:
        """
        Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
        process from the learned model outputs (most often the predicted noise).

        Args:
            model_output (`torch.FloatTensor`): direct output from learned diffusion model.
            timestep (`int`): current discrete timestep in the diffusion chain.
            sample (`torch.FloatTensor`):
                current instance of sample being created by diffusion process.
            eta (`float`): weight of noise for added noise in diffusion step.
            use_clipped_model_output (`bool`): if `True`, compute "corrected" `model_output` from the clipped
                predicted original sample. Necessary because predicted original sample is clipped to [-1, 1] when
                `self.config.clip_sample` is `True`. If no clipping has happened, "corrected" `model_output` would
                coincide with the one provided as input and `use_clipped_model_output` will have not effect.
            generator: random number generator.
            variance_noise (`torch.FloatTensor`): instead of generating noise for the variance using `generator`, we
                can directly provide the noise for the variance itself. This is useful for methods such as
                CycleDiffusion. (https://arxiv.org/abs/2210.05559)
            return_dict (`bool`): option for returning tuple rather than DDIMSchedulerOutput class

        Returns:
            [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] or `tuple`:
            [`~schedulers.scheduling_utils.DDIMSchedulerOutput`] if `return_dict` is True, otherwise a `tuple`. When
            returning a tuple, the first element is the sample tensor.

        """
        if self.num_inference_steps is None:
            raise ValueError(
                "Number of inference steps is 'None', you need to run 'set_timesteps' after creating the scheduler"
            )

        # See formulas (12) and (16) of DDIM paper https://arxiv.org/pdf/2010.02502.pdf
        # Ideally, read DDIM paper in-detail understanding

        # Notation (<variable name> -> <name in paper>
        # - pred_noise_t -> e_theta(x_t, t)
        # - pred_original_sample -> f_theta(x_t, t) or x_0
        # - std_dev_t -> sigma_t
        # - eta -> η
        # - pred_sample_direction -> "direction pointing to x_t"
        # - pred_prev_sample -> "x_t-1"

        # 1. get previous step value (=t-1)
        t = timesteps
        prev_t = t - self.config.num_train_timesteps // self.num_inference_steps

        t = t.view(-1, *([1] * (model_output.ndim - 1)))
        prev_t = prev_t.view(-1, *([1] * (model_output.ndim - 1)))

        # if model_output.shape[1] == sample.shape[1] * 2 and self.variance_type in ["learned", "learned_range"]:
        #     model_output, predicted_variance = torch.split(model_output, sample.shape[1], dim=1)
        # else:
        #     predicted_variance = None

        # 1. compute alphas, betas
        self.alphas_cumprod = self.alphas_cumprod.to(model_output.device)
        self.final_alpha_cumprod = self.final_alpha_cumprod.to(model_output.device)
        alpha_prod_t = self.alphas_cumprod[t]
        alpha_prod_t_prev = self.alphas_cumprod[torch.clip(prev_t, min=0)]
        alpha_prod_t_prev[prev_t < 0] = torch.tensor(1.0)

        beta_prod_t = 1 - alpha_prod_t

        # 3. compute predicted original sample from predicted noise also called
        # "predicted x_0" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf
        if self.config.prediction_type == "epsilon":
            pred_original_sample = (
                sample - beta_prod_t ** (0.5) * model_output
            ) / alpha_prod_t ** (0.5)
            pred_epsilon = model_output
        elif self.config.prediction_type == "sample":
            pred_original_sample = model_output
            pred_epsilon = (
                sample - alpha_prod_t ** (0.5) * pred_original_sample
            ) / beta_prod_t ** (0.5)
        elif self.config.prediction_type == "v_prediction":
            pred_original_sample = (alpha_prod_t**0.5) * sample - (
                beta_prod_t**0.5
            ) * model_output
            pred_epsilon = (alpha_prod_t**0.5) * model_output + (
                beta_prod_t**0.5
            ) * sample
        else:
            raise ValueError(
                f"prediction_type given as {self.config.prediction_type} must be one of `epsilon`, `sample`, or"
                " `v_prediction`"
            )

        # 4. Clip or threshold "predicted x_0"
        if self.config.thresholding:
            pred_original_sample = self._threshold_sample(pred_original_sample)
        elif self.config.clip_sample:
            pred_original_sample = pred_original_sample.clamp(
                -self.config.clip_sample_range, self.config.clip_sample_range
            )

        # 5. compute variance: "sigma_t(η)" -> see formula (16)
        # σ_t = sqrt((1 − α_t−1)/(1 − α_t)) * sqrt(1 − α_t/α_t−1)
        variance = (
            self._batch_get_variance(t, prev_t)
            .to(model_output.device)
            .view(*alpha_prod_t_prev.shape)
        )
        std_dev_t = eta * variance ** (0.5)

        if use_clipped_model_output:
            # the pred_epsilon is always re-derived from the clipped x_0 in Glide
            pred_epsilon = (
                sample - alpha_prod_t ** (0.5) * pred_original_sample
            ) / beta_prod_t ** (0.5)

        # 6. compute "direction pointing to x_t" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf
        pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (
            0.5
        ) * pred_epsilon

        # 7. compute x_t without "random noise" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf
        prev_sample = (
            alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction
        )

        return prev_sample
